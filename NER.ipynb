{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #4命名实体标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112e3f0d0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to make the code more readable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到最大值\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "# 将seq数据转换成index用于模型\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# 将seq数据转换成index用于模型，\n",
    "# 此处支持batch处理以及padding操作\n",
    "def prepare_sequence_with_batch_and_pad(seqs, to_ix, labels, tag_to_ix):\n",
    "    max_length = 0\n",
    "    idxs = []\n",
    "    label_idxs = []\n",
    "    for seq,label_seq in zip(seqs, labels):\n",
    "        if max_length <= len(seq):\n",
    "            max_length = len(seq)\n",
    "        idxs.append([to_ix[w] for w in seq])\n",
    "        label_idxs.append([tag_to_ix[w] for w in label_seq])\n",
    "    # 补完整\n",
    "    #print(max_length)\n",
    "    for i in range(len(idxs)):\n",
    "        idxs[i] = idxs[i] + [to_ix[\"<PAD>\"]] * (max_length-len(idxs[i]))\n",
    "        label_idxs[i] = label_idxs[i] + [tag_to_ix[\"<PAD>\"]] * (max_length-len(label_idxs[i]))\n",
    "        #print([tag_to_ix[\"<PAD>\"]] * (max_length-len(label_idxs[i])))\n",
    " \n",
    "    #print(idxs)\n",
    "    #print(label_idxs)\n",
    "    return torch.tensor(idxs, dtype=torch.long), torch.tensor(label_idxs, dtype=torch.long)\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "'''\n",
    "max_score维度是1，max_score(1,-1)的维度是（1，1），加上expand变成（1，tag_size）\n",
    "'''\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    # \n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        定义模型需要用到的参数\n",
    "        embedding_dim: 嵌入层维度\n",
    "        hidden_dim:    RNN单元的hidden维度\n",
    "        voacb_size:    词表大小\n",
    "        tag_to_ix:     标注->index的映射\n",
    "        tagset_size:   标注词表大小\n",
    "        '''\n",
    "        self.embedding_dim = embedding_dim   \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 使用了双向LSTM，因此hidden_dim要缩小至1/2\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # 将LSTM的输出映射到标注空间\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵的参数.  Entry i,j表示从j->i的转移\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 定义start和end标注的限制：\n",
    "        # 1）不可能从其他标注转移到start\n",
    "        # 2）不可能从end转移到其他标注\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        # 初始化RNN中的state\n",
    "        #self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self, sentence_len):\n",
    "        return (torch.randn(2, sentence_len, self.hidden_dim // 2),\n",
    "                torch.randn(2, sentence_len, self.hidden_dim // 2))\n",
    "\n",
    "    # 预测序列的分数\n",
    "    def _forward_alg(self, feats, tags):\n",
    "        scores = []\n",
    "        batchsize = tags.size(0)\n",
    "        length = tags.size(1)       \n",
    "        feats = feats.view(batchsize,length,-1)\n",
    "        for sen_id in range(batchsize):\n",
    "            # Do the forward algorithm to compute the partition function\n",
    "            # 初始化，全部为-10000\n",
    "            init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "            # START_TAG has all of the score.\n",
    "            init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "            # Wrap in a variable so that we will get automatic backprop\n",
    "            forward_var = init_alphas\n",
    "\n",
    "            # 遍历整句话的每个word\n",
    "            # feats: [-1, tag_size], -1表示batchsize * max_len\n",
    "            for feat in feats[sen_id]:\n",
    "                alphas_t = []  # The forward tensors at this timestep\n",
    "                for next_tag in range(self.tagset_size):\n",
    "                    # broadcast the emission score: it is the same regardless of\n",
    "                    # the previous tag\n",
    "                    # 用feat（分类层）对下一个标注的输出作为发射分数\n",
    "                    emit_score = feat[next_tag].view(\n",
    "                        1, -1).expand(1, self.tagset_size)\n",
    "\n",
    "                    # 第i个entry的转移分数，是从i->下一个标注的转移分数\n",
    "                    # trans_score所有其他标注到next_tag的分数\n",
    "                    trans_score = self.transitions[next_tag].view(1, -1) # (1,tag_size)\n",
    "                    # 第i个entry的next_tag_var，是在log-sum-exp计算之前，边i->下一个标注的值\n",
    "                    next_tag_var = forward_var + trans_score + emit_score\n",
    "                    # The forward variable for this tag is log-sum-exp of all the\n",
    "                    # scores.\n",
    "                    alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "                forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "            terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "            alpha = log_sum_exp(terminal_var)\n",
    "        \n",
    "        scores.append(alpha)\n",
    "        #print(alpha.size())\n",
    "        return sum(scores)\n",
    "\n",
    "    # 输入数据并且计算至分类层的输出，CRF算法中定义这里为发射矩阵\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden(len(sentence[0]))\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), sentence.size(1), -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(-1, self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    # 根据真实的标签算出score\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        scores = []\n",
    "        batchsize = tags.size(0)\n",
    "        length = tags.size(1)\n",
    "        feats = feats.view(batchsize,length,-1)\n",
    "        for sen_id in range(batchsize):\n",
    "            score = torch.zeros(1)\n",
    "            #temp = torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).expand(tags.size(0),1)\n",
    "            cur_tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags[sen_id].view(-1)])\n",
    "            #tags = tags.view(-1)\n",
    "            for i, feat in enumerate(feats[sen_id]):\n",
    "                # score等于当前score加上从当前tags转移至下一个tags的转移概率再加上下一个tags的分类层概率之和\n",
    "                score = score + \\\n",
    "                    self.transitions[cur_tags[i + 1], cur_tags[i]] + feat[cur_tags[i + 1]]\n",
    "            # 再加上从最后一个tags到stop的转移概率\n",
    "            score = score + self.transitions[self.tag_to_ix[STOP_TAG], cur_tags[-1]]\n",
    "        scores.append(score)\n",
    "        return sum(scores)\n",
    "\n",
    "    # 解码，得到预测的序列，以及预测序列的得分\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # 和forward_alg部分的初始化一样\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        # fowward_var在第 i step存储了第 i-1 step的viterbi结果\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            # 这里的feat和之前的虽然有区别，但是直接加上了emit的全部序列的值\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    \n",
    "    # 计算负对数似然函数\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        # feats作为经过LSTM特征抽取至分类层的抽象输出\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        # 预测序列的score\n",
    "        forward_score = self._forward_alg(feats, tags)\n",
    "        # 计算真实的score\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    \n",
    "    # 用在验证环节\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data_dict = {'sentence_list':[],'label_list':[]}\n",
    "for sentence_seq, label_seq in zip(raw_data[\"sentence_list\"], raw_data[\"label_list\"]):\n",
    "    newsentence_seq = ' '.join(sentence_seq.split(\" \")[1:-1])\n",
    "    newlabel_seq = ' '.join(label_seq.split(\" \")[1:-1])\n",
    "    new_train_data_dict['sentence_list'].append(newsentence_seq)\n",
    "    new_train_data_dict['label_list'].append(newlabel_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_level_sen = []\n",
    "tokens_level_label = []\n",
    "for sen, label in zip(raw_data['sentence_list'],raw_data['label_list']):\n",
    "    temp_sen = sen.split()\n",
    "    temp_label = label.split()\n",
    "    tokens_level_sen.append(temp_sen)\n",
    "    tokens_level_label.append(temp_label)\n",
    "new_train_data_dict['tokens_level_sen'] = tokens_level_sen\n",
    "new_train_data_dict['tokens_level_label'] = tokens_level_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('source/conll-corpora/CoNLL-2003/generated/generated_eng.train','rb') as f:\n",
    "    raw_data = pickle.load(f)\n",
    "with open('source/conll-corpora/CoNLL-2003/generated/label_vocab','rb') as f:\n",
    "    tag_to_ix, ix_to_tag = pickle.load(f)\n",
    "with open('source/conll-corpora/CoNLL-2003/generated/sentence_vocab','rb') as f:\n",
    "    word_to_ix, ix_to_word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sen = raw_data[\"tokens_level_sen\"]\n",
    "dataset_label = raw_data[\"tokens_level_label\"]\n",
    "dataset_sen, dataset_label = shuffle(dataset_sen, dataset_label)\n",
    "length = len(dataset_sen)\n",
    "train = 0.8\n",
    "valid = 0.1\n",
    "test = 0.1\n",
    "train_data_x, train_data_y = dataset_sen[:int(length*train)], dataset_label[:int(length*train)]\n",
    "valid_data_x, valid_data_y = dataset_sen[int(length*train):int(length*(train+valid))], dataset_label[int(length*train):int(length*(train+valid))]\n",
    "train_data_x, train_data_y = dataset_sen[:int(-length*test)], dataset_label[:int(-length*test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 38])\n",
      "torch.Size([32, 47])\n",
      "torch.Size([32, 39])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-2ecbbc35f329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Step 3.  forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Step 4. 计算loss、梯度以及更新参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-5b18802319bf>\u001b[0m in \u001b[0;36mneg_log_likelihood\u001b[0;34m(self, sentence, tags)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lstm_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# 预测序列的score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mforward_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_alg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;31m# 计算真实的score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mgold_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-5b18802319bf>\u001b[0m in \u001b[0;36m_forward_alg\u001b[0;34m(self, feats, tags)\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;31m# 用feat（分类层）对下一个标注的输出作为发射分数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     emit_score = feat[next_tag].view(\n\u001b[0;32m---> 72\u001b[0;31m                         1, -1).expand(1, self.tagset_size)\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0;31m# 第i个entry的转移分数，是从i->下一个标注的转移分数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<END>\"\n",
    "PAD_TAG = \"<PAD>\"\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "BATCHSIZE = 32\n",
    "EPOCH = 10\n",
    "# Make up some training data\n",
    "# training_data = [[(\n",
    "#     \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "#     \"B I I I O O O B I O O\".split()\n",
    "# ),(\n",
    "#     \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "#     \"B I I I O O O B I O O\".split()\n",
    "# ),(\n",
    "#     \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "#     \"B I I I O O O B I O O\".split()\n",
    "# ) ]]\n",
    "\n",
    "# tt_train = [\"the wall street journal reported today that apple corporation made money\".split(),\n",
    "#            \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "#            \"the wall street journal reported today that apple corporation made money\".split()]\n",
    "# tt_label = [\"B I I I O O O B I O O\".split(),\"B I I I O O O B I O O\".split(),\n",
    "#            \"B I I I O O O B I O O\".split()]\n",
    "\n",
    "#(\n",
    "#    \"georgia tech is a university in georgia\".split(),\n",
    "#    \"B I O O O O B\".split()\n",
    "#)\n",
    "# word_to_ix = {}\n",
    "# for sentence, tags in training_data[0]:\n",
    "#     for word in sentence:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "#tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# Check predictions before training\n",
    "# with torch.no_grad():\n",
    "#     precheck_sent = prepare_sequence_with_batch(tt_train, word_to_ix)\n",
    "#     precheck_tags = torch.tensor([[tag_to_ix[t] for t in tag_seq] for tag_seq in tt_label], dtype=torch.long)\n",
    "#     print(model(precheck_sent))\n",
    "\n",
    "total_step = len(train_data_x) // BATCHSIZE + 1\n",
    "for epo in range(EPOCH):  \n",
    "    \n",
    "    for step in range(total_step):\n",
    "        sentence = train_data_x[step * BATCHSIZE : (step + 1) * BATCHSIZE]\n",
    "        tags = train_data_y[step * BATCHSIZE : (step + 1) * BATCHSIZE]\n",
    "        # step 1.  清空梯度\n",
    "        model.zero_grad()\n",
    "\n",
    "        # step 2.  准备数据，包括word2index以及padding\n",
    "        sentence_in, targets = prepare_sequence_with_batch_and_pad(sentence, word_to_ix, tags, tag_to_ix)\n",
    "        print(sentence_in.size())\n",
    "        # Step 3.  forward\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # Step 4. 计算loss、梯度以及更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(step+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epo+1, EPOCH, step+1, total_step, loss.item())) \n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    valid_steps = 0\n",
    "    print(\"Validation...\")\n",
    "    for step in range(len(valid_data_x) // BATCHSIZE + 1):\n",
    "        sentence = valid_data_x[step * BATCHSIZE : (step + 1) * BATCHSIZE]\n",
    "        tags = valid_data_y[step * BATCHSIZE : (step + 1) * BATCHSIZE]  \n",
    "        \n",
    "        sentence_in = prepare_sequence_with_batch(sentence, word_to_ix)\n",
    "        targets = torch.tensor([[tag_to_ix[t] for t in tag_seq] for tag_seq in tags], dtype=torch.long)\n",
    "        \n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "        valid_loss += loss\n",
    "        valid_steps += 1\n",
    "    \n",
    "    avg_valid_loss = float('%.4f' %(valid_loss / valid_steps))\n",
    "    print ('Epoch [{}/{}], Valid_Loss: {:.4f}' \n",
    "           .format(epo+1, EPOCH, step+1, total_step, avg_valid_loss)) \n",
    "        \n",
    "        \n",
    "# 测试\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    print(model(precheck_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matrixs(predict, answers, index):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: A new loss function for discriminative tagging\n",
    "--------------------------------------------------------\n",
    "\n",
    "It wasn't really necessary for us to create a computation graph when\n",
    "doing decoding, since we do not backpropagate from the viterbi path\n",
    "score. Since we have it anyway, try training the tagger where the loss\n",
    "function is the difference between the Viterbi path score and the score\n",
    "of the gold-standard path. It should be clear that this function is\n",
    "non-negative and 0 when the predicted tag sequence is the correct tag\n",
    "sequence. This is essentially *structured perceptron*.\n",
    "\n",
    "This modification should be short, since Viterbi and score\\_sentence are\n",
    "already implemented. This is an example of the shape of the computation\n",
    "graph *depending on the training instance*. Although I haven't tried\n",
    "implementing this in a static toolkit, I imagine that it is possible but\n",
    "much less straightforward.\n",
    "\n",
    "Pick up some real data and do a comparison!\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
