{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "train_tsv = pd.read_csv(\"source/sentiment-analysis-on-movie-reviews/train.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test_tsv = pd.read_csv(\"source/sentiment-analysis-on-movie-reviews/test.tsv\",header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/156060 [00:00<?, ?it/s]/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'source'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "100%|██████████| 156060/156060 [00:22<00:00, 6903.07it/s]\n",
      "100%|██████████| 66292/66292 [00:09<00:00, 7190.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def Phrase2PureText(phrase):\n",
    "    \"\"\"\n",
    "    把评论转换成词序列\n",
    "    \"\"\"\n",
    "    # 去掉HTML标签\n",
    "    phrase_text = BeautifulSoup(phrase, \"html.parser\").get_text()\n",
    "    # 用正则取出纯字母部分\n",
    "    phrase_text = re.sub(\"[^a-zA-Z]\",\" \",phrase_text)\n",
    "    # 小写化所有词\n",
    "    cur_tokens = phrase_text.lower().split()\n",
    "    # 返回\n",
    "    return cur_tokens\n",
    "    \n",
    "# 预处理部分\n",
    "label = train_tsv[\"Sentiment\"]\n",
    "train_data = []\n",
    "for i in tqdm(range(len(train_tsv[\"Phrase\"]))):\n",
    "    train_data.append(' '.join(Phrase2PureText(train_tsv[\"Phrase\"][i])))\n",
    "test_data = []\n",
    "for i in tqdm(range(len(test_tsv[\"Phrase\"]))):\n",
    "    test_data.append(' '.join(Phrase2PureText(test_tsv[\"Phrase\"][i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump((train_data,list(label)),f)\n",
    "with open(\"testset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_data,f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 7371/222352 [00:00<00:02, 73707.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data len:  222352\n",
      "take data into counter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222352/222352 [00:01<00:00, 128305.27it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 708485.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find all words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 特征处理\n",
    "# dataset:包含了训练集/验证集的数据集\n",
    "def make_dict(dataset, mode=\"bag_of_word\", n=3,dict_size=1000):\n",
    "    \"\"\"\n",
    "    制作词字典\n",
    "    这里使用bag-aof-words进行实现\n",
    "    因此需要先制作所有词的字典\n",
    "    \"\"\"\n",
    "    print(\"take data into counter...\")\n",
    "    token_vocabs = Counter()\n",
    "    if mode == \"bag_of_word\":\n",
    "        for sentence in tqdm(dataset):\n",
    "            words = sentence.split(' ')\n",
    "            token_vocabs.update(words)\n",
    "    else:\n",
    "        for sentence in tqdm(dataset):\n",
    "            words = sentence.split(' ')\n",
    "            i = 3\n",
    "            n_grams_list = []\n",
    "            while(i < len(words)):\n",
    "                n_grams = ' '.join(words[i-3:i])\n",
    "                n_grams_list.append(n_grams)\n",
    "                i+=1\n",
    "            token_vocabs.update(n_grams_list)        \n",
    "    \n",
    "    index2word = {}\n",
    "    word2index = {}\n",
    "    print(\"find all words...\")\n",
    "    for idx, item in enumerate(tqdm(token_vocabs.most_common(dict_size))):\n",
    "        index2word[idx] = item[0]\n",
    "        word2index[item[0]] = idx\n",
    "    return token_vocabs, index2word, word2index\n",
    "\n",
    "all_data = train_data + test_data\n",
    "print(\"data len: \",len(all_data))\n",
    "token_vocabs1, index2word1, word2index1 = make_dict(all_data,dict_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156060/156060 [00:18<00:00, 8384.47it/s] \n",
      "100%|██████████| 66292/66292 [00:07<00:00, 8490.43it/s] \n"
     ]
    }
   ],
   "source": [
    "# 把train/test转换层index表示\n",
    "# 用bag_of_word\n",
    "def bag2word(dataset, word2index, dict_size=10000):\n",
    "    dataset2idx = []\n",
    "    for sentence in tqdm(dataset):\n",
    "        cur_seq = np.zeros([dict_size,])\n",
    "        for word in sentence.split(\" \"):\n",
    "            cur_idx = word2index.setdefault(word,None)\n",
    "            if cur_idx is not None:\n",
    "                cur_seq[cur_idx] += 1\n",
    "        dataset2idx.append(cur_seq)\n",
    "    return dataset2idx\n",
    "\n",
    "train_data2 = bag2word(train_data, word2index1)\n",
    "test_data2 = bag2word(test_data, word2index1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data2, label ,test_size = 0.2,random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchsize = 100\n",
    "model = LogisticRegression(penalty='l1')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5808342945021145"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
